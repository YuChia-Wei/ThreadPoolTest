## CPU 密集型任務效能測試報告 (無 RPS 限制)

**測試目標：** 移除 K6 的請求速率 (RPS) 限制，模擬真實的高併發場景，驗證在極限壓力下，不同執行緒池設定的效能表現。

**測試環境：** 4 核心 CPU，4GB 記憶體

**K6 測試設定：**
*   **虛擬使用者 (VUs):** 200
*   **測試時間:** 5 分鐘
*   **每秒請求數上限 (RPS):** **無限制**
*   **API 端點:** `/api/cpu-intensive-work?iterations=100`

---

### 測試案例分析

#### 1. Runtime-Control: .NET Runtime 動態控制 (無 RPS 限制)
*   **測試檔案:** `k6-result-runtime-set-thread-pool-4c-4G-cpu-test-norps.txt`

| 指標 | 結果 |
| :--- | :--- |
| **請求總數** | **1754** |
| **請求成功率** | 97.89% |
| **失敗率** | **2.10%** |
| **平均響應時間 (avg)** | 39.62s |
| **P95 響應時間** | 48.8s |
| **請求速率 (reqs/s)** | **4.91/s** |

**分析：**
在移除 RPS 限制後，此案例的失敗率從 0% 上升至 2.10%。這是一個關鍵的發現，它暴露了動態執行緒管理的內在成本：在應對突發的極高併發時，執行緒池需要時間來「熱身」和創建新的執行緒，這個延遲導致了少量的請求失敗。儘管如此，它仍然成功處理了絕大多數請求，並維持了所有測試中最高的請求速率 (4.91/s)，展現了優越的整體吞吐量和系統韌性。

---

#### 2. Preset-ThreadPool (MinThreads: 256, 無 RPS 限制)
*   **測試檔案:** `k6-result-preset-thread-pool-4C-4G-cpu-test-thread-256-norps.txt`

| 指標 | 結果 |
| :--- | :--- |
| **請求總數** | 1082 |
| **請求成功率** | 26.34% |
| **失敗率** | **73.65%** |
| **平均響應時間 (avg)** | 1m 1s |
| **P95 響應時間** | 1m 6s |
| **請求速率 (reqs/s)** | 2.95/s |

**分析：**
此案例的表現堪稱**災難性**。在無限制的併發壓力下，失敗率飆升至 **73.65%**。這強而有力地證明了先前的假設：為 CPU 密集型任務配置遠超 CPU 核心數的執行緒，會導致毀滅性的**上下文切換 (Context Switching) 開銷**。CPU 絕大部分時間都耗費在管理和切換這 256 個執行緒上，而無力執行實際的運算任務，導致系統有效處理能力崩潰，效能甚至遠不如讓 Runtime 自行管理。

---

### 最終結論

1.  **假設得到證實：** 本次無 RPS 限制的測試，決定性地證明了在 CPU 密集型場景下，**執行緒並非越多越好**。過度配置執行緒 (`MinThreads: 256`) 所帶來的上下文切換成本，是比執行緒不足更嚴重的效能殺手。

2.  **Runtime-Control 的韌性：** 儘管在極限壓力下會出現少量初始失敗，但 .NET Runtime 的動態管理機制展現了更強的整體吞吐能力和系統穩定性。對於通用的 CPU 密集型負載，依賴其內建的啟發式演算法通常是更安全、更高效的選擇。

3.  **效能調優的關鍵：** 真正的效能調優並非盲目地調高參數，而是要尋找「最佳點」。對於此類 CPU 密集型服務，理想的 `MinThreads` 設定應與 CPU 核心數緊密相關。下一步的優化方向應是測試較小的、接近核心數倍數的 `MinThreads` 值 (例如 8, 16, 32)，以找到能最大化 CPU 利用率且最小化上下文切換成本的那個「最佳點」。

**總而言之，本次測試明確指出，對於此應用場景，讓 .NET Runtime 控制執行緒池是遠比手動設定一個過高的值更優的策略。**
